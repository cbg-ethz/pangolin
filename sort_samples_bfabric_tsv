#!/usr/bin/env python3

import sys
import os
import glob
import io 
import re
import configparser
import argparse
import csv
import json
import yaml
import hashlib
import math
import datetime

# progress bar unicode
def bar(v, m=128):
	f=v&7
	return ('\u2588' * (v >> 3))+(chr(0x2590 - f) if f else '')+('\u00b7' * ((m-v) >> 3))


# parse command line
argparser = argparse.ArgumentParser(description="Fetch metadata from bfabric relying on the built-in metadata.tsv")
argparser.add_argument('-c', '--config', metavar='CONF', required=False,
	default='server.conf',
	type=str, dest='config', help="configuration file to load")
argparser.add_argument('-f', '--force', required=False,
	action='store_true', dest='force', help="Force overwriting any existing file when moving")
argparser.add_argument('-s', '--summary', required=False,
	action='store_true', dest='summary', help="Only display a summary of datasets, not an exhaustive list of all samples")
argparser.add_argument('-r', '--recent', metavar='ONLYAFTER', required=False,
	dest='recent', help="Only process batches whose date is posterior to the argument")
args = argparser.parse_args()


# Load defaults from config file
config = configparser.ConfigParser(strict=False) # non-strict: support repeated section headers
config.SECTCRE = re.compile(r'\[ *(?P<header>[^]]+?) *\]') # support spaces in section headers
with open(args.config) as f: config.read_string(f"""
[DEFAULT]
lab={os.path.splitext(os.path.basename(args.config))[0]}
basedir={os.getcwd()}
sampleset=sampleset
download=bfabric-downloads
link=--link
badlist=
fusedays=30
[_]
""" + f.read()) # add defaults + a pseudo-section "_" right before the ini file, to support bash-style section_header-less config files

lab=config['_']['lab'].strip("\"'")
'''name of the lab, to put in the batch YAML'''
basedir=config['_']['basedir'].strip("\"'")
'''base dircetory'''
expname=config['_']['expname'].strip("\"'")
'''projects name in SFTP'''
download=config['_']['download'].strip("\"'")
'''sub-directory to hold the unsorted downloaded datasets'''
sampleset=config['_']['sampleset'].strip("\"'")
'''sub-directory to hold the sorted samples set'''
link=config['_']['link'].strip("\"'")
'''
linking instead of copying ?
 --reflink for CoW filesystems (ZFS, BTRFS)
 --hardlink for most unix-like filesystems
'''
badlist=(config['_']['badlist'].strip("\"'").split(',')) if len(config['_']['badlist']) else list()
'''folders to skip'''
forcelist=(config['_']['forcelist'].strip("\"'").split(',')) if len(config['_']['forcelist']) else list()
'''folders to use even with missing order'''
fusedays=int(config['_']['fusedays'].strip("\"'"))
'''delay after which orders aren't considered for merging anymore'''

# glob all projects
if re.search('/p\d+/?$', expname):
	# project name included in the SFTP path, we don't need to scan
	projects=''
	extrapath=0
else:
	# whole storage in SFTP path, we need to scan for projects
	projects='p[0-9][0-9][0-9]*' 
	extrapath=1

# RegEx to parse some specific string
rxorder=re.compile('_(?P<order>o\d+)') # e.g.: MS556_COV19_o23657
rxrun=re.compile('^(?P<date>\d{6})_(?P<instr>\w+)_(?P<num>\d+)_(?:(?:0+-)|[AB])(?P<cell>\w+)$') # e.g.: 200430_M01761_0414_000000000-J3JCT or 201023_A00730_0259_BHTVCCDRXX
rxcell=re.compile('(?:\w+-)?(?P<cell>\w+)$') # e.g.: '000000000-CTT3D' or 'HTVCCDRXX'
rxsuffix=re.compile('(?:_S\d+)?(?:_L\d+)?$') # e.g.: ..._Plate_2_011120EG27_A4_S5_L001
rxfqext=re.compile('\.fastq\.gz$') 



################################
#                              #
#   Phase 1: Gather the data   #
#                              #
################################

# look for dataset files

# FastQC
fastqc={} # maps orders to FastQC directories (or in the absence of order number: checksum of the input_dataset)
fastqc_samples={} # maps which samples are present in which FastQC directory (some might be missing)
for d in glob.glob(os.path.join(basedir,download,projects,'Fastqc_*')):
	name = d.split(os.sep)[-1]
	if name in badlist:
		print(f"skipping {name} in bad list")
		continue

	t = os.path.join(d,'input_dataset.tsv'); 
	# FastQC_Result also listed dataset.tsv of Fastqc_ directories
	if not (os.path.isdir(os.path.join(d,'FastQC_Result')) and 
			os.path.isfile(t)):
		continue

	f = os.path.join(d.split(os.sep)[-1],'FastQC_Result')	# Holds the _fastqc.html files
	with open(t,'rt',encoding='utf-8') as tf:	# this file has the same content as the original experiment
		o=None	# keep tracking of the order -> FastQC mapping
		fastqc_samples[f]={}	# list of files (some files didn't get their respective FastQF
		for r in csv.DictReader(tf, dialect='excel-tab'):
			fastqc_samples[f][r['Name']] = True
			if (not o) and ('Order Id [B-Fabric]' in r):
				o = r['Order Id [B-Fabric]']

		if o:
			# match by Order Id, but not all have it
			fastqc[o]=f
			continue

	# match by (input_) dataset.tsv content
	md5_hash = hashlib.md5(usedforsecurity=False)
	with open(t,'rb') as tf:
		md5_hash.update(tf.read())
	fastqc[md5_hash.digest()]=f
    
# Samples
totsam=0
batches={}
for srch in glob.glob(os.path.join(basedir,download,projects,'*')):
	# (new style) look for bcl2fastq's json file
	j = os.path.join(srch, 'Stats','Stats.json')
	if not os.path.isfile(j):
		# (old style) look within the Reports generated by bcl2fastq
		j = os.path.join(srch, 'stats','Reports','Stats.json')
		if not os.path.isfile(j):
			continue
	pathparts = srch.split(os.sep)
	path = os.sep.join(pathparts)
	name = pathparts[-1]
	prj = pathparts[-2] if extrapath else ''

	if name in badlist:
		print(f"\x1b[35;1mskipping {name} in bad list\x1b[0m")
		continue

	order=name # default for corner cases when we don't have an actual order
	try:
		m=rxorder.search(name).groupdict()
		order=m['order']
	except:
		if name in forcelist:
			# no order but force processing
			print(f"\x1b[35;1m({name} in force list) \x1b[0m", end='')
			# search the dataset TSV in case we got a real order number
			with open(os.path.join(path,'dataset.tsv'),'rt',encoding='utf-8') as tf:
				r = next(csv.DictReader(tf, dialect='excel-tab'), None)
				if 'Order Id [B-Fabric]' in r:
					order = f"o{r['Order Id [B-Fabric]']}"
					print(order, end='')
			# otherwise we leave the above default (name)
			print()
		else:
			print(f"\x1b[31;1mcan't parse {name}\x1b[0m")
			continue

	########################################
	#                                      #
	#   Parse the Demultiplex stats JSON   #
	#                                      #
	########################################
	with open(j, 'rt') as f:
		stats = json.loads(f.read());

	# parse flowcell
	try:
		m=rxcell.search(stats['Flowcell']).groupdict()
		flowcell=m['cell']
	except:
		print(f"{name} cannot parse: {stats['Flowcell']}")
		continue

	# parse run folder
	runfolder=stats['RunId']
	try:
		m=rxrun.search(runfolder).groupdict()
		rundate=f"20{m['date']}" # NOTE runfolders are yymmdd, not yyyymmdd
		if flowcell != m['cell']:
			print(f"{name} Warning: cell missmatch: {flowcell} vs {m['cell']}")
	except:
		print(f"{name} cannot parse: {runfolder}")
		continue

	# parse information about reads
	lane={}
	for l in stats['ReadInfosForLanes']: # lane
		lanenum=l['LaneNumber']
		ends=rlen=0
		for r in l['ReadInfos']: # read phases (indexes, reads)
			if r['IsIndexedRead']: continue 

			# sanity check
			if rlen and rlen != r['NumCycles']:
				print(f"{name} Warning: read lenght changes from {rlen} to {r['NumCycles']} we currently only support symetric read lenghts")

			# gather info
			ends+=1
			if rlen < r['NumCycles']: rlen=r['NumCycles']
		
		# sanity check
		if ends < 1 or ends > 2:
			print(f"{name} Error: we currently only support single or paired ends, but found {ends} reads")

		lane[lanenum]={'ends': ends, 'rlen': rlen-1}

	# parse info about samples
	samples={}
	badyield=0
	badsamples=set()
	for l in stats['ConversionResults']: # lane
		lanenum=l['LaneNumber']
		ends=lane[lanenum]['ends']
		rlen=lane[lanenum]['rlen']

		for s in l['DemuxResults']: # sample in lane
			samname=s['SampleName']

			# filter out fastq files with zero reads
			if s['NumberReads'] == 0:
				badyield+=1;
				badsamples.add(samname)
				continue

			samples[samname]={'ends': ends, 'rlen': rlen}
			totsam+=1

	# Check readcounts
	if badyield:
		print(name, f"\x1b[33;1m{badyield} samples with bad yield !\x1b[0m", sep='\t')

	# Need multiple samples
	if len(samples) < 2:
		print(name, f"\x1b[31;1mOnly {len(samples)}!\x1b[0m", sep='\t')
		continue


	#
	#   Build per batch / samples data
	#

	t=os.path.join(path,'dataset.tsv')
	b={'name':name, 'prj': prj, 'path':path, 'dataset':t, 'flowcell': flowcell, 'runfolder': runfolder,'rundate':rundate,'samples':samples,'badyield':badyield,'badsamples':badsamples}

	# handle duplicate orders and fuse them if (close in time)
	if order in batches:
		key=f"{order}:{name}"

		days=abs(datetime.datetime.strptime(rundate, '%Y%m%d').date()-datetime.datetime.strptime(batches[order]['rundate'], '%Y%m%d').date()) //  datetime.timedelta(days=1)
		if days <= fusedays:
			print(name, f"\x1b[36;1m is dupe of order {order}\x1b[0m")
			batches[order]['dupe']=True
			b['appendto']=order
		else:
			print(name, f"\x1b[36;1mnot fusing {rundate}_{flowcell} with {order}'s {batches[order]['rundate']}_{batches[order]['flowcell']}: {days} days appart is more than {fusedays}\x1b[0m")
	else:
		key=order

	# Now, link with FastQC, based on dataset.tsv and...

	# ...based on order column if present ?
	with open(t,'rt',encoding='utf-8') as tf:	# this file has the same content as the original experiment
		r = next(csv.DictReader(tf, dialect='excel-tab'))
		if 'Order Id [B-Fabric]' in r:
			to = r['Order Id [B-Fabric]']
			# sanity check: is the folder the same?
			if to in fastqc:
				b['fastqc']=fastqc[to]
				if to[0] != 'o':
					to = f"o{to}"
				if to != order:
					print(name, f"\x1b[31;1mFolder order {order} vs dataset.tsv order {to}\x1b[0m")
			batches[key]=b
			continue

	# ...match by (input_) dataset.tsv content
	md5_hash = hashlib.md5(usedforsecurity=False)
	with open(t,'rb') as tf:
		md5_hash.update(tf.read())
	md5=md5_hash.digest()
	if md5 in fastqc:
		b['fastqc']=fastqc[md5]
	batches[key]=b


################################
#                              #
#   Phase 2: Output the thing  #
#                              #
################################

# create sampleset directory if missing
if not os.path.isdir(os.path.join(basedir,sampleset)):
	try:
		os.mkdir(os.path.join(basedir,sampleset), mode=0o770)
	except FileExistsError:
		pass


# shell script file with all moving instructions inside
sh=open(os.path.join(basedir,sampleset,'movedatafiles.sh'), 'wt')

# generic header: only for stand-alone files.
print(r'''
link='%(link)s'

# Helper
fail() {
	printf '\e[31;1mArgh: %%s\e[0m\n'	"$1"	1>&2
	[[ -n "$2" ]] && echo "$2" 1>&2
	exit 1
}

warn() {
	printf '\e[33;1mArgh: %%s\e[0m\n'	"$1"	1>&2
	[[ -n "$2" ]] && echo "$2" 1>&2
}

ALLOK=1
X() {
	ALLOK=0
}

# sanity checks
[[ -d '%(sampleset)s' ]] || fail 'No sampleset directory:' '%(sampleset)s'
[[ -d '%(download)s' ]] || fail 'No download directory:' '%(download)s'
''' % {'link':link,'sampleset':sampleset,'download':download}, file=sh)

lastbar=-1
cursam=0
otsv={}
for b in batches:
	name=batches[b]['name']
	prj=batches[b]['prj']
	rundate=batches[b]['rundate']
	flowcell=batches[b]['flowcell']

	# skip older
	if args.recent:
		if rundate < args.recent:
			totsam -= len(batches[b]['samples'])
			#print(f"Skipping {rundate} {order} {flowcell}")
			continue

	# either classic '20201120_JDNB4' or merged '20201124_o23391'
	dupe=False
	if 'dupe' in batches[b]:
		dupe=True
		order=b
		batch=f"{rundate}_{order}"
	elif 'appendto' in batches[b]:
		dupe=True
		order = batches[b]['appendto']
		rundate = batches[order]['rundate']
		batch=f"{rundate}_{order}"
	else:
		order=b
		batch=f"{rundate}_{flowcell}"

	print(r"[[ -d '%(download)s/%(prj)s/%(id)s' ]] || fail 'Not a directory:' '%(download)s/%(prj)s/%(id)s'" % {'download':download,'prj':prj,'id':name}, file=sh)
	qcdir=None
	if (not dupe) and ('fastqc' in batches[b]):
		qcdir=batches[b]['fastqc']
		print(r"[[ -d '%(download)s/%(prj)s/%(qc)s' ]] || fail 'No download directory:' '%(download)s/%(prj)s/%(qc)s'" % {'download':download,'prj':prj,'qc': qcdir}, file=sh)

	# patch file exist ?
	patchmap = { }
	patchtsv=os.path.join(basedir,sampleset,f"patch.{prj.removeprefix('p')}.{order.removeprefix('o')}.tsv")
	if os.path.isfile(patchtsv):
		print(f"\x1b[32;1mpatching {order} {name} with {patchtsv}\x1b[0m")
		with open(patchtsv,'rt',encoding='utf-8', newline='') as pf:
			patchmap = { old:new for (old,new,*r) in csv.reader(pf, delimiter="\t") }

	# batch TSV file and info YAML
	runfolder=None
	if batch not in otsv:
		otsv[batch]=tsv=open(os.path.join(basedir,sampleset,f'samples.{batch}.tsv'), 'wt')
		bfabfolder=batches[b]['name']
		runfolder=batches[b]['runfolder']
		fastqcfolder=batches[b]['fastqc'] if 'fastqc' in batches[b] else None
		badsamples=batches[b]['badsamples']
	else:
		tsv=otsv[batch]
		bfabfolder=[batches[b]['name'],batches[order]['name']]
		runfolder=[batches[b]['runfolder'],batches[order]['runfolder']]
		fastqcfolder=[]
		if 'fastqc' in batches[b]:
			fastqcfolder+=[batches[b]['fastqc']]
		if 'fastqc' in batches[order]:
			fastqcfolder+=[batches[order]['fastqc']]
		if len(fastqcfolder) == 0:
			fastqcfolder=None
		badsamples=batches[b]['badsamples'].intersection(batches[order]['badsamples'])
	with open(os.path.join(basedir,sampleset,f'batch.{batch}.yaml'), 'wt') as yml:
		print(yaml.dump({'type':'bfabric','lab':lab,'order':order,'project':prj,'name':name,'fused':dupe,'runfolder':runfolder,'fastqcfolder':fastqcfolder,'folder':bfabfolder}, sort_keys=False), file=yml)
	if len(badsamples):
		with open(os.path.join(basedir,sampleset,f'missing.{batch}.txt'), 'wt') as missing:
			missing.writelines("%s\n" % bad for bad in badsamples)

	with open(batches[b]['dataset'],'rt',encoding='utf-8') as tf:
		for r in csv.DictReader(tf, dialect='excel-tab'):
			# progress
			prgbar=math.floor(cursam*128/totsam)
			if lastbar != prgbar:
				print(f"echo -ne '\\r[{bar(prgbar)}]\\r'", file=sh)
				lastbar=prgbar
			cursam+=1

			# match read TSV to known samples
			fulname=samname=r['Name']
			if samname in batches[b]['samples']:
				batsamname=samname
			else:
				olen=len(samname)
				# try removing typical trailing stuff: Sample num, Lane num
				samname=rxsuffix.sub('', samname)
				if samname in batches[b]['samples']:
					batsamname=samname
				else:
					# try if one of the batch's sample has a name which is a subset
					slen=len(samname)
					mlen=0
					matchname=None
					for batsamname in batches[b]['samples']:
						tlen=len(batsamname)
						if tlen < slen:
							if (samname[:tlen] == batsamname) and (mlen < tlen):
								mlen=tlen
								matchname=batsamname
						else:
							if (samname == batsamname[:slen]) and (mlen < slen):
								mlen=slen
								matchname=batsamname
					if matchname != None:
						print(f"{batches[b]['name']} {samname} fuzzy matched to {matchname}")
						batsamname=matchname
					else:
						print(f"{batches[b]['name']} Can't match {samname}")
						continue
			if dupe:
				fulname=f"{fulname}_{flowcell}"

			# use patch map to adjust name
			if samname in patchmap:
				samname = patchmap[samname]

			# files
			ends=batches[b]['samples'][batsamname]['ends']
			rlen=batches[b]['samples'][batsamname]['rlen']
			r1=r['Read1 [File]'].split(os.sep)[-1]
			if ends==2:
				r2=r['Read2 [File]'].split(os.sep)[-1]

			# tsv line
			if ('appendto' not in batches[b]) or (batsamname not in batches[order]['samples']):
				print(samname, batch, rlen, sep='\t', file=tsv)

			# move script
			print(r'''
mkdir --mode=0770 -p "%(sampleset)s/%(sname)s/%(batch)s/"{raw_data,extracted_data}
cp -v%(force)s ${link} '%(download)s/%(prj)s/%(id)s/%(read)s' '%(sampleset)s/%(sname)s/%(batch)s/raw_data/%(destname)s'||X''' % {'force': ('f' if args.force else ''),'download':download,'prj':prj,'id':name,'sname':samname,'batch':batch,'sampleset':sampleset,'read':r1,'destname':f"{fulname}_R1.fastq.gz"}, file=sh)
			if ends==2:
				print(r"cp -v%(force)s ${link} '%(download)s/%(prj)s/%(id)s/%(read)s' '%(sampleset)s/%(sname)s/%(batch)s/raw_data/%(destname)s'||X" % {'force': ('f' if args.force else ''),'download':download,'prj':prj,'id':name,'sname':samname,'batch':batch,'sampleset':sampleset,'read':r2,'link':link,'destname':f"{fulname}_R2.fastq.gz"}, file=sh)
			if qcdir and fulname in fastqc_samples[qcdir]:
				fqc=rxfqext.sub('_fastqc.html',r1)
				print(r"cp -v%(force)s ${link} '%(download)s/%(prj)s/%(fastqc)s/%(fqc)s' '%(sampleset)s/%(sname)s/%(batch)s/extracted_data/R1_fastqc.html'||X" %{'force': ('f' if args.force else ''),'download':download,'prj':prj,'fastqc':qcdir,'fqc':fqc,'sampleset':sampleset,'sname':samname,'batch':batch}, file=sh)
				if ends==2:
					fqc=rxfqext.sub('_fastqc.html',r2)
					print(r"cp -v%(force)s ${link} '%(download)s/%(prj)s/%(fastqc)s/%(fqc)s' '%(sampleset)s/%(sname)s/%(batch)s/extracted_data/R2_fastqc.html'||X" %{'force': ('f' if args.force else ''),'download':download,'prj':prj,'fastqc':qcdir,'fqc':fqc,'sampleset':sampleset,'sname':samname,'batch':batch}, file=sh)

print(f"""
echo -e '\\r\\e[K[{bar(128)}] done.'
if (( ALLOK )); then
	echo All Ok
	exit 0
else
	echo Some errors
	exit 1
fi;
""", file=sh)

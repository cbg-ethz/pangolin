[ _ ]
# labs: SFTP synchronisation
# (except in case of abnormally slow operation, sync can be left active)
skipsync=
# labs: sample sorting and import
# (swith 0 for labs whose sequences we aren't currently importing, e.g. due to problems)
lab=( ['fgcz']=0 )
# NOTE: h2030 isn't sequencing as the number of cases is lower.
# pass '--force' to force overwriting any existing file when moving
sort_force=--force
# base wastewater dir
wwdir=/app
# base directory
basedir=${wwdir}/workdir
# sub-directory of openbis download for rights fixing
download=openbis-downloads
# sub-directory to hold the sorted samples set
# working sub-directory
# linking instead of copying ?
#  --reflink for CoW filesystems (ZFS, BTRFS)
#  --hardlink for most unix-like filesystems
link=--link
# Installation directory of miniconda3
baseconda=${wwdir}/miniconda3
# File containing the Rsync password
rsync_pass=/home/bs-pangolin/.ssh/rsync.pass.euler

# group on the storage (inside download and sampleset)
# storgrp=bsse-covid19-pangolin@d.ethz.ch
storgrp=bs-pangolin-group
# parallel copy jobs
parallel=8
# parallel backup copy jobs
parallelpull=4
# whereto push the sequences at the end
#releasedir=/links/shared/covid19-pangolin/pangolin/consensus_data/batch/
#releasedir=$(realpath $(pwd)/batch)
# timeout before rsync considers the transfer failed in seconds
rsynctimeout=2000
# SSH connection timeout
contimeout=300
# IO timeout
iotimeout=300
# suspend jobs submission in case of problems
donotsubmit=0
# skip or run ShoRAH step of V-pipe
run_shorah=0
# mail
mailfrom='Automation-carillon<bs-pangolin@ethz.ch>'
mailto=( carrara@nexus.ethz.ch, ivan.topolsky@bsse.ethz.ch )
# timeout for the whole automation loop
runtimeout=7200
# timeout to wait for 'sync' command: 0: no timeout, "": default, otherwise seconds
synctimeout=0

# username used on the cluster (while we're switching over)
cluster_user=bs-pangolin
cluster=euler.ethz.ch
clusterdir="/cluster/project/pangolin"
working=working
sampleset="sampleset"
# rsyncd module to sync from the remote status folder
remote_status=remote_status
# rsyncd module to sync from the remote bfabric sync folder
bfabric_downloads=bfabric-downloads
# rsyncd module to sync to the remote VILOCA work folder
work_viloca=work-viloca
#rsyncd module to sync to the remote uploader work folder
work_uploader=work-uploader

# BSSE folder backups config
# Local directory for the backups
backupdir=XXX
# Subdirectory for the VILOCA backups
viloca_backup_subdir=viloca_results
# Subdirectory for the uploader results
uploader_backup_subdir=uploader_archive
# Subdirectory for the sync results (sync from the SFTP servers of the raw data providers)
sync_backup_subdir=sync_backup

# VILOCA-specific configuration
run_viloca=1
donotsubmit_viloca=0
# Local VILOCA work directory
viloca_basedir=${basedir}/viloca
# Remote VILOCA base directory
remote_viloca_basedir=${clusterdir}/work-viloca
# Remote processing directory for VILOCA
viloca_processing=SARS-CoV-2-wastewater-sample-processing-VILOCA
# Remote results subdirectory for VILOCA
viloca_results=results
# Name of the sample list file for VILOCA
viloca_samples=samples.csv
# Staging file for VILOCA sample list
viloca_staging=${viloca_samples}.staging

# Upload-specific configuration
# Filename of the list of samples to upload
uploaderlist=batches_to_upload.tsv
# Remote work directory for the uploader
uploader_workdir=work-uploader
# Daily quota of files to upload on SPSP
upload_number_quota=500
# Daily quota of size to upload on SPSP in GB
upload_size_quota=100
# Remote temporary directory for files that should be dropped or overwritten between runs
uploader_tempdir=${uploader_workdir}/temp
# Remote staging directory with the structure expected by the SPSP SendCrypt CLI tool
uploader_staging=${uploader_workdir}/staging
# Remote sub directory of the staging directory where the files to upload should be copied
uploader_target=${uploader_staging}/viruses/wastewater-uploads
# Remote archival directory for the upload files
archive=${uploader_workdir}/archive
# Remote location of the full list of uploaded samples
uploader_uploaded=${uploader-workdir}/all_uploaded.tsv



#remote_batman="ssh -ni ${HOME}/.ssh/id_ed25519_batman -l ${cluster_user} euler.ethz.ch --"
#remote_belfry="ssh -ni ${HOME}/.ssh/id_ed25519_belfry bs-bewi09.ethz.ch --"
